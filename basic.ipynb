{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"saved_models\"\n",
    "\n",
    "# Seeding for random operations\n",
    "seed = 42\n",
    "main_rng = random.PRNGKey(seed)\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from dataset import image_to_numpy, numpy_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = partial(image_to_numpy, dataset_name=\"CIFAR10\")\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      partial(image_to_numpy, dataset_name=\"CIFAR10\")\n",
    "                                     ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(seed))\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For initial testing part of dataset\n",
    "SMALL_DATASET_SIZE = 0.3\n",
    "train_set_small = torch.utils.data.Subset(train_set, range(int(SMALL_DATASET_SIZE * len(train_set))))\n",
    "val_set_small = torch.utils.data.Subset(val_set, range(int(SMALL_DATASET_SIZE * len(val_set))))\n",
    "test_set_small = torch.utils.data.Subset(test_set, range(int(SMALL_DATASET_SIZE * len(test_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a set of data loaders that we can use for training and validation\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                               batch_size=128,\n",
    "                               shuffle=True,\n",
    "                               drop_last=True,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=8,\n",
    "                               persistent_workers=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_set,\n",
    "                               batch_size=128,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=4,\n",
    "                               persistent_workers=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_set,\n",
    "                               batch_size=128,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=4,\n",
    "                               persistent_workers=True)\n",
    "\n",
    "# small dataloaders\n",
    "\n",
    "small_train_loader = torch.utils.data.DataLoader(train_set_small,\n",
    "                               batch_size=128,\n",
    "                               shuffle=True,\n",
    "                               drop_last=True,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=8,\n",
    "                               persistent_workers=True)\n",
    "small_val_loader   = torch.utils.data.DataLoader(val_set_small,\n",
    "                               batch_size=128,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=4,\n",
    "                               persistent_workers=True)\n",
    "small_test_loader  = torch.utils.data.DataLoader(test_set_small,\n",
    "                               batch_size=128,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=4,\n",
    "                               persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#check images variance\n",
    "imgs, _ = next(iter(train_loader))\n",
    "print(\"Batch mean\", imgs.mean(axis=(0,1,2)))\n",
    "print(\"Batch std\", imgs.std(axis=(0,1,2)))\n",
    "np.testing.assert_almost_equal(imgs.mean(axis=(0,1,2)), np.array([0,0,0]), decimal=1)\n",
    "np.testing.assert_almost_equal(imgs.std(axis=(0,1,2)), np.array([1,1,1]), decimal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from utils import visualize_img\n",
    "\n",
    "NUM_IMAGES = 4\n",
    "images = [train_dataset[idx][0] for idx in range(NUM_IMAGES)]\n",
    "orig_images = [Image.fromarray(train_dataset.data[idx]) for idx in range(NUM_IMAGES)]\n",
    "orig_images = [test_transform(img) for img in orig_images]\n",
    "\n",
    "visualize_img(orig_images+images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_modules import TrainerModule, TrainerModuleBatch\n",
    "from network import GoogleNet\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "googlenet_trainer = TrainerModuleBatch(model_class=GoogleNet,\n",
    "                                model_name=\"GoogleNet\",\n",
    "                                model_hparams={\"num_classes\": 10,\n",
    "                                                \"act_fn\": nn.relu},\n",
    "                                optimizer_name=\"adamw\",\n",
    "                                checkpoint_dir=CHECKPOINT_PATH,\n",
    "                                optimizer_hparams={\"lr\": 1e-3,\n",
    "                                                    \"weight_decay\": 1e-4},\n",
    "                                exmp_imgs=jax.device_put(next(iter(train_loader))[0]),)\n",
    "googlenet_trainer.train_model(small_train_loader, small_val_loader, num_epochs=num_epochs)\n",
    "googlenet_trainer.load_model()\n",
    "val_acc = googlenet_trainer.eval_model(small_val_loader)\n",
    "test_acc = googlenet_trainer.eval_model(small_test_loader)\n",
    "val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "googlenet_trainer_layer = TrainerModule(model_class=GoogleNet,\n",
    "                                model_name=\"GoogleNet_layer\",\n",
    "                                model_hparams={\"num_classes\": 10,\n",
    "                                               \"batch_norm\": False,\n",
    "                                                \"act_fn\": nn.relu},\n",
    "                                optimizer_name=\"adamw\",\n",
    "                                checkpoint_dir=CHECKPOINT_PATH,\n",
    "                                optimizer_hparams={\"lr\": 1e-3,\n",
    "                                                    \"weight_decay\": 1e-4},\n",
    "                                exmp_imgs=jax.device_put(next(iter(train_loader))[0]),)\n",
    "googlenet_trainer_layer.train_model(small_train_loader, small_val_loader, num_epochs=num_epochs)\n",
    "googlenet_trainer_layer.load_model()\n",
    "val_acc = googlenet_trainer_layer.eval_model(small_val_loader)\n",
    "test_acc = googlenet_trainer_layer.eval_model(small_test_loader)\n",
    "val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "googlenet_trainer_gelu = TrainerModuleBatch(model_class=GoogleNet,\n",
    "                                model_name=\"GoogleNet_gelu\",\n",
    "                                model_hparams={\"num_classes\": 10,\n",
    "                                                \"act_fn\": nn.gelu},\n",
    "                                optimizer_name=\"adamw\",\n",
    "                                checkpoint_dir=CHECKPOINT_PATH,\n",
    "                                optimizer_hparams={\"lr\": 1e-3,\n",
    "                                                    \"weight_decay\": 1e-4},\n",
    "                                exmp_imgs=jax.device_put(next(iter(train_loader))[0]),)\n",
    "googlenet_trainer_gelu.train_model(small_train_loader, small_val_loader, num_epochs=num_epochs)\n",
    "googlenet_trainer_gelu.load_model()\n",
    "val_acc = googlenet_trainer_gelu.eval_model(small_val_loader)\n",
    "test_acc = googlenet_trainer_gelu.eval_model(small_test_loader)\n",
    "val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import ResNet, ResNetBlock, PreActResNetBlock\n",
    "\n",
    "num_epochs = 2\n",
    "resnet_trainer = TrainerModuleBatch(model_class=ResNet,\n",
    "                                model_name=\"ResNet\",\n",
    "                                model_hparams={\"num_classes\": 10,\n",
    "                                                \"c_hidden\": (16, 32, 64),\n",
    "                                                \"num_blocks\": (3, 3, 3),\n",
    "                                                \"act_fn\": nn.relu,\n",
    "                                                \"block_class\": ResNetBlock},\n",
    "                                optimizer_name=\"SGD\",\n",
    "                                checkpoint_dir=CHECKPOINT_PATH,\n",
    "                                optimizer_hparams={\"lr\": 0.1,\n",
    "                                                    \"momentum\": 0.9,\n",
    "                                                    \"weight_decay\": 1e-4},\n",
    "                                exmp_imgs=jax.device_put(next(iter(train_loader))[0]),)\n",
    "resnet_trainer.train_model(small_train_loader, small_val_loader, num_epochs=num_epochs)\n",
    "resnet_trainer.load_model()\n",
    "val_acc = resnet_trainer.eval_model(small_val_loader)\n",
    "test_acc = resnet_trainer.eval_model(small_test_loader)\n",
    "val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "preactresnet_trainer = TrainerModuleBatch(model_class=ResNet,\n",
    "                                model_name=\"ResNet\",\n",
    "                                model_hparams={\"num_classes\": 10,\n",
    "                                                \"c_hidden\": (16, 32, 64),\n",
    "                                                \"num_blocks\": (3, 3, 3),\n",
    "                                                \"act_fn\": nn.relu,\n",
    "                                                \"block_class\": PreActResNetBlock},\n",
    "                                optimizer_name=\"SGD\",\n",
    "                                checkpoint_dir=CHECKPOINT_PATH,\n",
    "                                optimizer_hparams={\"lr\": 0.1,\n",
    "                                                    \"momentum\": 0.9,\n",
    "                                                    \"weight_decay\": 1e-4},\n",
    "                                exmp_imgs=jax.device_put(next(iter(train_loader))[0]),)\n",
    "preactresnet_trainer.train_model(small_train_loader, small_val_loader, num_epochs=num_epochs)\n",
    "preactresnet_trainer.load_model()\n",
    "val_acc = preactresnet_trainer.eval_model(small_val_loader)\n",
    "test_acc = preactresnet_trainer.eval_model(small_test_loader)\n",
    "val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import DenseNet\n",
    "\n",
    "num_epochs = 2\n",
    "densenet_trainer = TrainerModuleBatch(model_class=DenseNet,\n",
    "                                model_name=\"DenseNet\",\n",
    "                                model_hparams={\"num_classes\": 10,\n",
    "                                                \"num_layers\": [6, 6, 6, 6],\n",
    "                                                \"bn_size\": 2,\n",
    "                                                \"act_fn\": nn.relu,\n",
    "                                                \"growth_rate\": 16},\n",
    "                                optimizer_name=\"adamw\",\n",
    "                                checkpoint_dir=CHECKPOINT_PATH,\n",
    "                                optimizer_hparams={\"lr\": 1e-3,\n",
    "                                                    \"weight_decay\": 1e-4},\n",
    "                                exmp_imgs=jax.device_put(next(iter(train_loader))[0]),)\n",
    "densenet_trainer.train_model(small_train_loader, small_val_loader, num_epochs=num_epochs)\n",
    "densenet_trainer.load_model()\n",
    "val_acc = densenet_trainer.eval_model(small_val_loader)\n",
    "test_acc = densenet_trainer.eval_model(small_test_loader)\n",
    "val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fc71b447a20366fdd03b1e6c5af2922442f0a362a7e69937da6485d63918b4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
